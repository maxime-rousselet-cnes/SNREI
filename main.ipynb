{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyogrio\n",
    "\n",
    "def shapefile2fingerprint(shapefile: str|gpd.GeoDataFrame, buffer:int=0, proj: str|int=3857) -> xr.DataArray:\n",
    "    if isinstance(shapefile, str):\n",
    "        shapefile_gpd = pyogrio.read_dataframe(shapefile)\n",
    "    elif isinstance(shapefile, gpd.GeoDataFrame):\n",
    "        shapefile_gpd = shapefile\n",
    "    \n",
    "    if buffer > 0:\n",
    "        shapefile_gpd = shapefile_gpd.to_crs(proj).buffer(buffer*1e3).to_crs(4326)\n",
    "    \n",
    "    grid_gpd = pyogrio.read_dataframe(grid_fn)\n",
    "    \n",
    "    # Compute the intersection between the fingerprint and the grid\n",
    "    grid_clip_gpd = gpd.clip(grid_gpd, shapefile_gpd)\n",
    "    grid_clip_gpd['area'] = grid_clip_gpd.to_crs(proj).area\n",
    "    \n",
    "    grid_gpd['ratio'] = 0\n",
    "    grid_gpd['area'] = grid_gpd.to_crs(proj).area\n",
    "\n",
    "    grid_gpd.loc[grid_clip_gpd.index, 'ratio'] = grid_clip_gpd['area'] / grid_gpd['area']\n",
    "    fingerprint_xr = grid_gpd.loc[:, ['lat','lon','ratio']].reset_index(drop=True).set_index(['lat', 'lon']).to_xarray().sortby('lon', ascending=True).sortby('lat', ascending=False).ratio\n",
    "    return fingerprint_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing_functions import *\n",
    "from importResults import missing_month, _X_grace_END_GRID_MEAN_NoGIA, _X_mascons_CSR, _X_mascons_JPL\n",
    "import re\n",
    "\n",
    "here_dir = os.path.abspath(os.getcwd())\n",
    "\n",
    "ddk_filter_file = os.path.join(model_dir, \"2_Filtre_DDK\", \"DDK7.ddk\")\n",
    "le_filter_file = os.path.join(model_dir, \"1_Filtre_LE\", \"coefficients_LE_FILTER_DDK7.coef\")\n",
    "love_number_file = os.path.join(model_dir, \"3_Load_Love\", \"Load_Love2_CF_deg0.dat\")\n",
    "\n",
    "trend_xr = grid_xr2Poly2nSin(_X_grace_END_GRID_MEAN_NoGIA).trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leakage Continental & Océanique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddk_filter_file = os.path.join(model_dir, \"2_Filtre_DDK\", \"DDK7.ddk\")\n",
    "le_filter_file = os.path.join(model_dir, \"1_Filtre_LE\", \"coefficients_LE_FILTER_DDK7.coef\")\n",
    "save_map(trend_xr, vmin=-5, vmax=5, title=\"GRACE/-FO trend\", show=True, noTitle=False, byYear=True, engine=\"pygmt\")\n",
    "ocean_signal = 0.2  # cm/an\n",
    "ocean_xr = (get_oceans() > 0.7) * 1\n",
    "mask_non_oceanic_signal = -ocean_xr + 1 + (abs(trend_xr * ocean_xr) - 0.8 > 0) * 1\n",
    "mask_oceanic_signal = -mask_non_oceanic_signal + 1\n",
    "save_map(mask_non_oceanic_signal, vmin=0, vmax=1, title=\"Masque Océan / Continents(+séismes)\", show=True, noTitle=False, byYear=True)\n",
    "oceanic_signal = mask_oceanic_signal * ocean_xr\n",
    "oceanic_signal_filtered = grace_df2grid_xr(\n",
    "    grace_df_LOBE_EDGE(\n",
    "        grace_dfDecorelation(grid_xr2grace_df(oceanic_signal, isGCoeff=True), file_ddk_filter=ddk_filter_file), le_filter_file=le_filter_file\n",
    "    ),\n",
    "    isEWH=True,\n",
    ")\n",
    "oceanic_leakage = (oceanic_signal_filtered - oceanic_signal) * (-ocean_xr + 1)\n",
    "save_map(oceanic_leakage, vmin=-1, vmax=1, title=\"Oceanic Leakage\", show=True, noTitle=False, byYear=True)\n",
    "continental_signal = (-ocean_xr + 1) * trend_xr\n",
    "continental_signal_filtered = grace_df2grid_xr(\n",
    "    grace_df_LOBE_EDGE(\n",
    "        grace_dfDecorelation(grid_xr2grace_df(continental_signal, isGCoeff=True), file_ddk_filter=ddk_filter_file), le_filter_file=le_filter_file\n",
    "    ),\n",
    "    isEWH=True,\n",
    ")\n",
    "contiental_leakage = (continental_signal_filtered - continental_signal) * ocean_xr\n",
    "save_map(contiental_leakage, vmin=-1, vmax=1, title=\"Continental Leakage\", show=True, noTitle=False, byYear=True)\n",
    "save_map(contiental_leakage + oceanic_leakage, vmin=-1, vmax=1, title=\"Oceanic & Continental Leakage\", show=True, noTitle=False, byYear=True)\n",
    "save_map(\n",
    "    trend_xr - (contiental_leakage + oceanic_leakage),\n",
    "    vmin=-5,\n",
    "    vmax=5,\n",
    "    title=\"GRACE/-FO trend corrected for Oceanic & Continental Leakage\",\n",
    "    show=True,\n",
    "    noTitle=False,\n",
    "    byYear=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signal océanique à considérer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_signal = 0.2  # cm/an\n",
    "ocean_xr = (get_oceans() > 0.7) * 1\n",
    "\n",
    "mask_non_oceanic_signal = -ocean_xr + 1 + (abs(trend_xr * ocean_xr) - 0.8 > 0) * 1\n",
    "mask_oceanic_signal = -mask_non_oceanic_signal + 1\n",
    "\n",
    "save_map(mask_non_oceanic_signal, show=True, engine=\"cartopy\")\n",
    "save_map(trend_xr * mask_non_oceanic_signal, cmap=cmap_5, show=True, engine=\"pygmt\")\n",
    "\n",
    "dates = []\n",
    "for year in range(2000, 2030):\n",
    "    for month in range(1, 13):\n",
    "        dates.append(pda.to_datetime(\"{:4d}-{:02d}-15\".format(year, month), format=\"%Y-%m-%d\"))\n",
    "ocean_signal_xr = []\n",
    "for date in tqdm(dates, desc=\"Etalement sur le temps\"):\n",
    "    g_date = gregorianDate2decimalYear(pda.to_datetime(date))\n",
    "    ocean_signal_xr.append(g_date * mask_oceanic_signal * ocean_signal)\n",
    "ocean_signal_xr = xr.concat(ocean_signal_xr, dates).rename({\"concat_dim\": \"date\"})\n",
    "ocean_signal_xr = grid_xr2rem_time_mean_xr(ocean_signal_xr)\n",
    "\n",
    "\n",
    "print(\"###################    1ère méthode    ###################\")\n",
    "\n",
    "non_oceanic_signal = _X_grace_END_GRID_MEAN_NoGIA * mask_non_oceanic_signal + ocean_signal_xr * mask_oceanic_signal\n",
    "print(f\"Signal continental + signal océanique artificiel {ocean_signal}cm/an\")\n",
    "save_map(grid_xr2Poly2nSin(non_oceanic_signal).trend, cmap=cmap_5, engine=\"pygmt\", show=True)\n",
    "\n",
    "non_oceanic_signal = grid_xr2rem_time_mean_xr(\n",
    "    grace_df2grid_xr(\n",
    "        grace_df_LOBE_EDGE(grace_dfDecorelation(grid_xr2grace_df(non_oceanic_signal, isGCoeff=True), ddk_filter_file), le_filter_file), isEWH=True\n",
    "    )\n",
    ")\n",
    "print(f\"Signal continental + signal océanique artificiel {ocean_signal}cm/an filtré avec DDK7 + LE\")\n",
    "save_map(grid_xr2Poly2nSin(non_oceanic_signal).trend, cmap=cmap_5, engine=\"pygmt\", show=True)\n",
    "\n",
    "non_oceanic_signal = non_oceanic_signal - ocean_signal_xr * mask_oceanic_signal - _X_grace_END_GRID_MEAN_NoGIA * (-ocean_xr + 1)\n",
    "print(f\"Signaux résiduels dans les océans non issus des océans\")\n",
    "save_map(grid_xr2Poly2nSin(non_oceanic_signal).trend, cmap=cmap_5, engine=\"pygmt\", show=True)\n",
    "\n",
    "print(f\"Données corrigées des signaux non océaniques (leakage + EQ)\")\n",
    "oceanic_signal_xr = _X_grace_END_GRID_MEAN_NoGIA - non_oceanic_signal\n",
    "save_map(grid_xr2Poly2nSin(oceanic_signal_xr).trend, cmap=cmap_5, engine=\"pygmt\", show=True)\n",
    "\n",
    "write_netCDF(oceanic_signal_xr, os.path.join(here_dir, \"oceanic_signal_xr.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_signal = 0.2  # cm/an\n",
    "ocean_xr = (get_oceans() > 0.7) * 1\n",
    "\n",
    "mask_non_oceanic_signal = -ocean_xr + 1 + (abs(trend_xr * ocean_xr) - 0.8 > 0) * 1\n",
    "mask_oceanic_signal = -mask_non_oceanic_signal + 1\n",
    "\n",
    "save_map(mask_non_oceanic_signal, show=True, engine=\"cartopy\")\n",
    "save_map(trend_xr * mask_non_oceanic_signal, cmap=cmap_5, show=True, engine=\"pygmt\")\n",
    "\n",
    "dates = []\n",
    "for year in range(2000, 2030):\n",
    "    for month in range(1, 13):\n",
    "        dates.append(pda.to_datetime(\"{:4d}-{:02d}-15\".format(year, month), format=\"%Y-%m-%d\"))\n",
    "ocean_signal_xr = []\n",
    "for date in tqdm(dates, desc=\"Etalement sur le temps\"):\n",
    "    g_date = gregorianDate2decimalYear(pda.to_datetime(date))\n",
    "    ocean_signal_xr.append(g_date * mask_oceanic_signal * ocean_signal)\n",
    "ocean_signal_xr = xr.concat(ocean_signal_xr, dates).rename({\"concat_dim\": \"date\"})\n",
    "ocean_signal_xr = grid_xr2rem_time_mean_xr(ocean_signal_xr)\n",
    "\n",
    "print(\"###################    2ème méthode    ###################\")\n",
    "non_oceanic_signal = _X_grace_END_GRID_MEAN_NoGIA * mask_non_oceanic_signal + ocean_signal_xr * mask_oceanic_signal\n",
    "save_map(grid_xr2Poly2nSin(non_oceanic_signal).trend, cmap=cmap_5, engine=\"pygmt\", show=True)\n",
    "\n",
    "non_oceanic_signal = grid_xr2rem_time_mean_xr(\n",
    "    grace_df2grid_xr(\n",
    "        grace_df_LOBE_EDGE(\n",
    "            grace_dfDecorelation(grid_xr2grace_df(non_oceanic_signal, love_number_file=love_number_file), ddk_filter_file), le_filter_file\n",
    "        ),\n",
    "        love_number_file=love_number_file,\n",
    "    )\n",
    ")\n",
    "save_map(grid_xr2Poly2nSin(non_oceanic_signal).trend, cmap=cmap_5, engine=\"pygmt\", show=True)\n",
    "non_oceanic_signal = non_oceanic_signal - ocean_signal_xr * mask_oceanic_signal - _X_grace_END_GRID_MEAN_NoGIA * (-ocean_xr + 1)\n",
    "save_map(grid_xr2Poly2nSin(non_oceanic_signal).trend, cmap=cmap_5, engine=\"pygmt\", show=True)\n",
    "\n",
    "oceanic_signal_xr = _X_grace_END_GRID_MEAN_NoGIA - non_oceanic_signal\n",
    "save_map(grid_xr2Poly2nSin(oceanic_signal_xr).trend, cmap=cmap_5, engine=\"pygmt\", show=True)\n",
    "\n",
    "write_netCDF(oceanic_signal_xr, os.path.join(here_dir, \"oceanic_signal_xr_deg0.nc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export des trends océaniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_files = list(filter(lambda x: \"TS\" in x and \"oc_buffer\" in x and \"oceanic_signal\" in x and \"lat65\" not in x, os.listdir(here_dir)))\n",
    "\n",
    "data = {}\n",
    "for TS_file in TS_files:\n",
    "    (solution, buffer) = re.findall(r\"TS_(\\S+)_oc_buffer_(\\d*)km.pickle\", TS_file)[0]\n",
    "    buffer = int(buffer)\n",
    "    if solution not in data:\n",
    "        data[solution] = {}\n",
    "    data[solution][buffer] = read_pickle(TS_file)\n",
    "\n",
    "for solution in data.keys():\n",
    "    date_start = pda.to_datetime(\"2005-01-01\")\n",
    "    date_end = pda.to_datetime(\"2018-12-31\")\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    for buffer in [50, 100, 200, 500, 700, 1000]:\n",
    "        period = (data[solution][buffer].date >= date_start) & (data[solution][buffer].date < date_end)\n",
    "        # period = (data[solution][buffer].date>=pda.to_datetime(\"2005-01-01\")) & (data[solution][buffer].date<pda.to_datetime(\"2015-12-31\"))\n",
    "\n",
    "        data_t = data[solution][buffer]\n",
    "        TS = data_t.values\n",
    "        DT = data_t.date.values\n",
    "        DY = np.array(list(map(gregorianDate2decimalYear, DT)))\n",
    "\n",
    "        (p0, p1, p2, p3, a1, phi1, a2, phi2) = model_8p(TS, DT)\n",
    "        print(\"Buffer : {} km\".format(buffer))\n",
    "        print(\"p1 : {:.02f} mm/yr\".format(p1 * 10))\n",
    "        print(\"p2 : {:.02f} mm/yr²\\n\".format(p2 * 10))\n",
    "\n",
    "        signal_to_remove = a1 * np.cos(2 * np.pi * DY - phi1 * np.pi / 6) + a2 * np.cos(4 * np.pi * DY - phi2 * np.pi / 3)\n",
    "\n",
    "        TS = (TS - signal_to_remove) * 10  # Retrait du signal plus conversion en mm\n",
    "        TS -= np.mean(TS[:6])\n",
    "\n",
    "        period_index = ((DT >= date_start) * 1 * (DT <= date_end) * 1) == 1\n",
    "        (p0, p1) = model_2p(TS[period_index], DT[period_index])\n",
    "        if buffer == 700:\n",
    "            fit_values = p0 + p1 * DY\n",
    "            plt.plot(DT, fit_values, label=\"Linear fit (700 km) \\n a={:.02f}mm/an b={:.02f}mm\".format(p1, p0))\n",
    "        label = \"{} km - \".format(buffer) + \"$\\dot{\\sigma}$\" + \" = {:.02f} mm/yr\".format(p1)\n",
    "\n",
    "        plt.plot(DT, TS, label=label)\n",
    "\n",
    "    # TODO : Supprimer les signaux liés aux séismes !\n",
    "\n",
    "    plt.xlim((np.min(DT), np.max(DT)))\n",
    "    plt.ylim((-2, 40))\n",
    "    ylim = plt.ylim()\n",
    "    for i, period in enumerate(missing_month):\n",
    "        date_period_min = min(period)\n",
    "        date_period_max = max(period)\n",
    "\n",
    "        if i == 0:\n",
    "            plt.fill_betweenx(ylim, date_period_min, date_period_max, alpha=0.3, zorder=-1, color=\"C0\", label=\"Missing data\", edgecolor=None)\n",
    "        else:\n",
    "            plt.fill_betweenx(ylim, date_period_min, date_period_max, alpha=0.3, zorder=-1, color=\"C0\", edgecolor=None)\n",
    "\n",
    "    # plt.title('Sum of the signals over the whole oceans depending of the buffer to the coast')\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"mean SLE [mm]\")\n",
    "    plt.title(\"Period : {:02d}-{:4d} - {:02d}-{:4d}\".format(date_start.month, date_start.year, date_end.month, date_end.year))\n",
    "    plt.legend()\n",
    "    plt.savefig(\"mean_SLR_{}.png\".format(solution), dpi=600)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
